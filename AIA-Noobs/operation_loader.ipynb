{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "289d658f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models_pytorch\n",
      "  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 244 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (7.2.0)\n",
      "Collecting pretrainedmodels==0.7.4\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 217 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting timm==0.4.12\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[K     |████████████████████████████████| 376 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.7.1\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (4.62.2)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.10.0+cu111)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.9.0+cu111)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.10.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.21.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.16.0)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=78878e82e5c58475e168cdaac183bf2d29d6f3abf9117c1042b966adefa2590e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=a5cf011361fcac046da69483d978e3a045a4476cbfe5d5f836b275788a58e408\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.0 timm-0.4.12\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123f490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "import os\n",
    "import imgaug\n",
    "import imgaug.augmenters as iaa\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_dir, config):\n",
    "        self.mask_paths = sorted(glob(os.path.join(video_dir, 'segmentation/*.png')))\n",
    "        self.img_paths = [p.replace('segmentation', 'rgb') for p in self.mask_paths]\n",
    "        self.img_h = config['img_h']\n",
    "        self.img_w = config['img_w']\n",
    "        self.aug = config['aug']\n",
    "        self.seq = iaa.Sequential([\n",
    "            iaa.Fliplr(0.5),  # 50% horizontal flip\n",
    "            iaa.Affine(\n",
    "                rotate=(-15, 15),\n",
    "                shear=(-10, 10),\n",
    "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "            ),\n",
    "        ])\n",
    "        self.config = config\n",
    "        self._init_img_preprocess_fn(config)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mask_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_paths[idx])[:, :, ::-1]        \n",
    "        img = self.preprocess(img)\n",
    "        mask = self.preprocess_mask(mask)\n",
    "\n",
    "        if self.aug:\n",
    "            img, mask = self.seq(image=img, segmentation_maps=mask)\n",
    "\n",
    "        return self._to_torch_tensor(img, mask)\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        img = cv2.resize(img, (self.img_w, self.img_h))\n",
    "        return img\n",
    "    \n",
    "    def preprocess_mask(self, mask):\n",
    "        mask = imgaug.augmentables.segmaps.SegmentationMapsOnImage(mask.astype(np.int8), \n",
    "                                                                   shape=mask.shape)\n",
    "        mask = mask.resize((self.img_h, self.img_w))\n",
    "        return mask\n",
    "\n",
    "    def _init_img_preprocess_fn(self, config):\n",
    "        model_type = config['model_type']\n",
    "        if model_type == 'UNet' and config[model_type]['encoder']['pretrained']:\n",
    "            transform = T.Compose([\n",
    "                T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "            # img = transform(img)\n",
    "        elif model_type == 'smp':\n",
    "            encoder_name = config[model_type]['encoder_name']\n",
    "            pretrained = config[model_type]['pretrained'] # used pretrained\n",
    "            transform = get_preprocessing_fn(encoder_name, pretrained='imagenet')\n",
    "        else:\n",
    "            raise ValueError('Not implemented model type preprocess fn')\n",
    "        self.transform = transform\n",
    "\n",
    "    def _to_torch_tensor(self, img, mask):\n",
    "        model_type = self.config['model_type']\n",
    "        if model_type == 'UNet':\n",
    "            img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1) / 255.\n",
    "            img = self.transform(img)\n",
    "\n",
    "        elif model_type == 'smp':\n",
    "            img = img / 255.\n",
    "            img = self.transform(img)\n",
    "            img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1)\n",
    "        else:\n",
    "            raise ValueError('Not implemented model type preprocess fn')\n",
    "        mask = mask.get_arr()  # to np\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _build_seg_dataset(video_dirs, config):\n",
    "    datasets = []\n",
    "    for video_dir in tqdm(video_dirs):\n",
    "        ds = SegDataset(video_dir, config)\n",
    "        datasets.append(ds)\n",
    "    return torch.utils.data.ConcatDataset(datasets)\n",
    "\n",
    "def build_seg_datasets(config):\n",
    "    data_config = config['data']\n",
    "    root_dir = data_config['root_dir']\n",
    "    train_video_dirs = pd.read_csv(data_config['train_csv'])['video_dir'].tolist()\n",
    "    val_video_dirs = pd.read_csv(data_config['val_csv'])['video_dir'].tolist()\n",
    "    train_video_dirs = _add_root_dir(root_dir, train_video_dirs)\n",
    "    val_video_dirs = _add_root_dir(root_dir, val_video_dirs)\n",
    "    print('# of Videos: ', len(train_video_dirs), len(val_video_dirs))\n",
    "    train_ds = _build_seg_dataset(train_video_dirs, config)\n",
    "    val_ds = _build_seg_dataset(val_video_dirs, config)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "    \n",
    "\n",
    "def _add_root_dir(root_dir, dirs):\n",
    "    return [os.path.join(root_dir, p) for p in dirs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
