{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ecdf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/jovyan/.local/lib/python3.7/site-packages (0.4.12)\n",
      "Collecting timm\n",
      "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 372 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.0+cu111)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.9.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (3.10.0.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (7.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.2)\n",
      "Installing collected packages: timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.4.12\n",
      "    Uninstalling timm-0.4.12:\n",
      "      Successfully uninstalled timm-0.4.12\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "segmentation-models-pytorch 0.3.0 requires timm==0.4.12, but you have timm 0.6.7 which is incompatible.\u001b[0m\n",
      "Successfully installed timm-0.6.7\n"
     ]
    }
   ],
   "source": [
    "!pip install timm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd3aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config path \n",
      " config: ../configs/config.yaml \n",
      " config_workspace: ../configs/config_aia.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import load_config\n",
    "class Arguments: pass\n",
    "args=Arguments()\n",
    "args.config = '../configs/config.yaml'\n",
    "args.config_workspace = '../configs/config_aia.yaml'\n",
    "\n",
    "cfg=load_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15aeb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e89c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name=cfg[\"smp\"][\"encoder_name\"]\n",
    "num_cls=cfg[\"num_cls\"]\n",
    "# https://smp.readthedocs.io/en/latest/_modules/segmentation_models_pytorch/decoders/unetplusplus/model.html#UnetPlusPlus\n",
    "# model = smp.UnetPlusPlus(encoder_name=encoder_name,\n",
    "#                          encoder_depth=5,\n",
    "#                          encoder_weights='imagenet',\n",
    "#                          decoder_use_batchnorm=True,\n",
    "#                          decoder_channels=(256, 128, 64, 32, 16),\n",
    "#                          decoder_attention_type=None,\n",
    "#                          in_channels=3,\n",
    "#                          classes=num_cls, activation=None, aux_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a6ac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timm-efficientnet-b2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d56fe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 320, 320])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model(torch.zeros(1,3,320,320))\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712d8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9522202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 40, 40])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.interpolate(pred,scale_factor=pow(0.5,3),mode=\"bilinear\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b35d6de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder', 'decoder', 'segmentation_head')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*zip(*model.named_children())][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83dd30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List\n",
    "\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.base import (\n",
    "    SegmentationModel,\n",
    "    SegmentationHead,\n",
    "    ClassificationHead,\n",
    ")\n",
    "\n",
    "\n",
    "class UnetPlusPlusCGMDecoder(smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder):\n",
    "    # output depth      ^         ^            ^\n",
    "    #                  |          |            |\n",
    "    #         f0 -> out(0,0) -> out(0,1) -> *out(0,2) ->\n",
    "    #         ^     ^^          ^^^         ^^^^\n",
    "    #         f1 -> latent(1,1)-> out(1,2) \n",
    "    #         ^     ^^            ^^^\n",
    "    #         f2 -> latent(2,2) \n",
    "    #         ^     ^^\n",
    "    # input-> f3\n",
    "    def forward(self, *features):\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        # start building dense connections\n",
    "        dense_x = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(self.depth - layer_idx):\n",
    "                if layer_idx == 0:\n",
    "                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](features[depth_idx], features[depth_idx + 1])\n",
    "                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n",
    "                else:\n",
    "                    dense_l_i = depth_idx + layer_idx                    \n",
    "                    cat_features = [dense_x[f\"x_{didx}_{dense_l_i}\"] for didx in range(depth_idx + 1, dense_l_i + 1)]\n",
    "                    cat_features = torch.cat(cat_features + [features[dense_l_i + 1]], dim=1)\n",
    "                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[f\"x_{depth_idx}_{dense_l_i}\"](\n",
    "                        dense_x[f\"x_{depth_idx}_{dense_l_i-1}\"], cat_features\n",
    "                    )\n",
    "                    \n",
    "        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](dense_x[f\"x_{0}_{self.depth-1}\"])\n",
    "        return [dense_x[f\"x_{0}_{d}\"] for d in range(self.depth+1)]\n",
    "\n",
    "\n",
    "class UnetPlusPlusCGM(SegmentationModel):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1807.10165\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "        self.encoder_depth=encoder_depth\n",
    "        \n",
    "        self.decoder = UnetPlusPlusCGMDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "        )\n",
    "        \n",
    "        self.pooling_heads=[torch.nn.AvgPool3d(\n",
    "            kernel_size=[chs//decoder_channels[-1],1,1]\n",
    "        ) for chs in decoder_channels]\n",
    "        \n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3)\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"unetplusplus-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "    def forward(self, x):\n",
    "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "\n",
    "        self.check_input_shape(x)\n",
    "\n",
    "        features = self.encoder(x)\n",
    "        decoder_output = self.decoder(*features)\n",
    "        # print([o.shape for o in decoder_output])\n",
    "        pooled_output = [h(raw_mask) for h,raw_mask in zip(self.pooling_heads,decoder_output)]\n",
    "        # print([o.shape for o in pooled_output])\n",
    "        masks=[*map(self.segmentation_head,pooled_output)]\n",
    "        \n",
    "        if self.classification_head is not None:\n",
    "            labels = self.classification_head(features[-1])\n",
    "            return masks, labels\n",
    "\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b13fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetPlusPlusCGM(encoder_name=encoder_name,\n",
    "                         encoder_depth=5,\n",
    "                         encoder_weights='imagenet',\n",
    "                         decoder_use_batchnorm=True,\n",
    "                         decoder_channels=(256, 128, 64, 32, 16),\n",
    "                         decoder_attention_type=None,\n",
    "                         in_channels=3,\n",
    "                         classes=num_cls, activation=None, aux_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b1af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model(torch.zeros(1,3,320,320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b252660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 10, 20, 20]),\n",
       " torch.Size([1, 10, 40, 40]),\n",
       " torch.Size([1, 10, 80, 80]),\n",
       " torch.Size([1, 10, 160, 160]),\n",
       " torch.Size([1, 10, 320, 320])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eeff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.upscore6 = nn.Upsample(scale_factor=32,mode='bilinear')###\n",
    "self.upscore5 = nn.Upsample(scale_factor=16,mode='bilinear')\n",
    "self.upscore4 = nn.Upsample(scale_factor=8,mode='bilinear')\n",
    "self.upscore3 = nn.Upsample(scale_factor=4,mode='bilinear')\n",
    "self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f6bf32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class And1:\n",
    "    def __init__(self):\n",
    "        for i in range(1,5):\n",
    "            self.__dict__[f\"upscore{i}\"]=nn.Upsample(scale_factor=2*i,mode='bilinear')\n",
    "a1=And1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eaa0fdbb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.4870e-03, -2.7763e-02, -9.0263e-02,  ...,  1.6601e-01,\n",
       "            1.6265e-01,  1.6098e-01],\n",
       "          [-1.6006e-01, -1.1437e-01, -2.2983e-02,  ...,  1.6297e-01,\n",
       "            1.7167e-01,  1.7603e-01],\n",
       "          [-4.8715e-01, -2.8757e-01,  1.1158e-01,  ...,  1.5690e-01,\n",
       "            1.8972e-01,  2.0612e-01],\n",
       "          ...,\n",
       "          [-3.2354e-01, -2.5185e-01, -1.0848e-01,  ...,  2.5266e-02,\n",
       "            1.2078e-01,  1.6854e-01],\n",
       "          [-5.1421e-01, -3.7356e-01, -9.2257e-02,  ...,  1.5943e-01,\n",
       "            3.1755e-01,  3.9661e-01],\n",
       "          [-6.0955e-01, -4.3442e-01, -8.4145e-02,  ...,  2.2651e-01,\n",
       "            4.1593e-01,  5.1065e-01]],\n",
       "\n",
       "         [[-3.1131e-02,  1.2800e-01,  4.4626e-01,  ...,  3.0857e-01,\n",
       "            5.4100e-01,  6.5721e-01],\n",
       "          [-1.2427e-01, -4.8210e-04,  2.4709e-01,  ...,  2.0827e-01,\n",
       "            4.8761e-01,  6.2728e-01],\n",
       "          [-3.1054e-01, -2.5745e-01, -1.5127e-01,  ...,  7.6639e-03,\n",
       "            3.8083e-01,  5.6742e-01],\n",
       "          ...,\n",
       "          [-4.1731e-01, -3.0234e-01, -7.2399e-02,  ...,  1.5530e-01,\n",
       "            3.3414e-01,  4.2356e-01],\n",
       "          [-3.3454e-01, -2.7510e-01, -1.5622e-01,  ...,  1.1749e-01,\n",
       "            2.4979e-01,  3.1594e-01],\n",
       "          [-2.9316e-01, -2.6149e-01, -1.9813e-01,  ...,  9.8590e-02,\n",
       "            2.0762e-01,  2.6213e-01]],\n",
       "\n",
       "         [[ 7.2301e-01,  7.2243e-01,  7.2127e-01,  ...,  1.1507e-01,\n",
       "            9.1261e-02,  7.9357e-02],\n",
       "          [ 7.3473e-01,  7.4968e-01,  7.7960e-01,  ...,  2.5943e-01,\n",
       "            2.1301e-01,  1.8980e-01],\n",
       "          [ 7.5816e-01,  8.0419e-01,  8.9624e-01,  ...,  5.4815e-01,\n",
       "            4.5651e-01,  4.1068e-01],\n",
       "          ...,\n",
       "          [ 6.5746e-01,  6.7425e-01,  7.0785e-01,  ...,  4.6189e-01,\n",
       "            3.0854e-01,  2.3186e-01],\n",
       "          [ 4.5679e-01,  5.3837e-01,  7.0153e-01,  ...,  5.3364e-01,\n",
       "            4.0404e-01,  3.3923e-01],\n",
       "          [ 3.5646e-01,  4.7043e-01,  6.9837e-01,  ...,  5.6952e-01,\n",
       "            4.5178e-01,  3.9291e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.5936e-02,  2.5210e-02,  2.4750e-01,  ...,  9.2128e-03,\n",
       "           -3.7768e-02, -6.1258e-02],\n",
       "          [-1.4034e-01, -4.3449e-02,  1.5034e-01,  ...,  7.3467e-02,\n",
       "           -1.2362e-02, -5.5276e-02],\n",
       "          [-2.4916e-01, -1.8077e-01, -4.3975e-02,  ...,  2.0198e-01,\n",
       "            3.8450e-02, -4.3313e-02],\n",
       "          ...,\n",
       "          [-2.1036e-01, -2.1893e-01, -2.3609e-01,  ..., -1.5653e-01,\n",
       "           -1.5938e-01, -1.6081e-01],\n",
       "          [-3.8616e-01, -3.6707e-01, -3.2889e-01,  ..., -2.1544e-01,\n",
       "           -1.8237e-01, -1.6583e-01],\n",
       "          [-4.7406e-01, -4.4113e-01, -3.7529e-01,  ..., -2.4490e-01,\n",
       "           -1.9386e-01, -1.6834e-01]],\n",
       "\n",
       "         [[-1.4983e-01, -3.9037e-01, -8.7145e-01,  ..., -7.2927e-01,\n",
       "           -5.9277e-01, -5.2452e-01],\n",
       "          [-1.1250e-01, -4.1137e-01, -1.0091e+00,  ..., -8.1736e-01,\n",
       "           -6.9642e-01, -6.3595e-01],\n",
       "          [-3.7838e-02, -4.5336e-01, -1.2844e+00,  ..., -9.9354e-01,\n",
       "           -9.0373e-01, -8.5882e-01],\n",
       "          ...,\n",
       "          [-3.5750e-01, -5.8448e-01, -1.0384e+00,  ..., -1.1047e+00,\n",
       "           -8.0818e-01, -6.5992e-01],\n",
       "          [-4.7950e-02, -2.2013e-01, -5.6448e-01,  ..., -7.9735e-01,\n",
       "           -6.0574e-01, -5.0993e-01],\n",
       "          [ 1.0683e-01, -3.7952e-02, -3.2750e-01,  ..., -6.4368e-01,\n",
       "           -5.0451e-01, -4.3493e-01]],\n",
       "\n",
       "         [[-6.7839e-01, -6.0374e-01, -4.5443e-01,  ..., -2.3281e-01,\n",
       "           -9.8827e-02, -3.1836e-02],\n",
       "          [-8.1548e-01, -7.1793e-01, -5.2284e-01,  ..., -1.2606e-01,\n",
       "            6.1803e-02,  1.5573e-01],\n",
       "          [-1.0897e+00, -9.4632e-01, -6.5966e-01,  ...,  8.7448e-02,\n",
       "            3.8306e-01,  5.3087e-01],\n",
       "          ...,\n",
       "          [-8.5127e-01, -7.2971e-01, -4.8658e-01,  ...,  8.0515e-02,\n",
       "            4.5322e-01,  6.3958e-01],\n",
       "          [-6.4530e-01, -5.6056e-01, -3.9110e-01,  ...,  1.8155e-01,\n",
       "            5.7191e-01,  7.6710e-01],\n",
       "          [-5.4231e-01, -4.7599e-01, -3.4336e-01,  ...,  2.3206e-01,\n",
       "            6.3126e-01,  8.3086e-01]]]], grad_fn=<UpsampleBilinear2DBackward1>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.upscore1(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e839b21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 40, 40])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample2x(pred[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21bc295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087df8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import smppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89847320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8cb8a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eca_swinnext26ts_256',\n",
       " 'swin_base_patch4_window7_224',\n",
       " 'swin_base_patch4_window7_224_in22k',\n",
       " 'swin_base_patch4_window12_384',\n",
       " 'swin_base_patch4_window12_384_in22k',\n",
       " 'swin_large_patch4_window7_224',\n",
       " 'swin_large_patch4_window7_224_in22k',\n",
       " 'swin_large_patch4_window12_384',\n",
       " 'swin_large_patch4_window12_384_in22k',\n",
       " 'swin_small_patch4_window7_224',\n",
       " 'swin_tiny_patch4_window7_224',\n",
       " 'swinnet26t_256',\n",
       " 'swinnet50ts_256']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models(\"*swin*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ef312c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convit_base',\n",
       " 'convit_small',\n",
       " 'convit_tiny',\n",
       " 'levit_128',\n",
       " 'levit_128s',\n",
       " 'levit_192',\n",
       " 'levit_256',\n",
       " 'levit_384',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_224_in21k',\n",
       " 'vit_base_patch16_224_miil',\n",
       " 'vit_base_patch16_224_miil_in21k',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch32_224',\n",
       " 'vit_base_patch32_224_in21k',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_r26_s32_224',\n",
       " 'vit_base_r50_s16_224',\n",
       " 'vit_base_r50_s16_224_in21k',\n",
       " 'vit_base_r50_s16_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50_224_in21k',\n",
       " 'vit_base_resnet50_384',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_huge_patch14_224_in21k',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_224_in21k',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_224',\n",
       " 'vit_large_patch32_224_in21k',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_large_r50_s32_224',\n",
       " 'vit_large_r50_s32_224_in21k',\n",
       " 'vit_large_r50_s32_384',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_patch16_224_in21k',\n",
       " 'vit_small_patch16_384',\n",
       " 'vit_small_patch32_224',\n",
       " 'vit_small_patch32_224_in21k',\n",
       " 'vit_small_patch32_384',\n",
       " 'vit_small_r26_s32_224',\n",
       " 'vit_small_r26_s32_224_in21k',\n",
       " 'vit_small_r26_s32_384',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s16_224',\n",
       " 'vit_tiny_patch16_224',\n",
       " 'vit_tiny_patch16_224_in21k',\n",
       " 'vit_tiny_patch16_384',\n",
       " 'vit_tiny_r_s16_p8_224',\n",
       " 'vit_tiny_r_s16_p8_224_in21k',\n",
       " 'vit_tiny_r_s16_p8_384']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models(\"*vit*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "874c2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "457df2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72eefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "swinnet26t_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0580d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ns-51548356.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/tf_efficientnet_b6_ns-51548356.pth\n"
     ]
    }
   ],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name='tu-tf_efficientnet_b6_ns',\n",
    "                                     encoder_depth=5,\n",
    "                                     encoder_weights='imagenet',\n",
    "                                     decoder_use_batchnorm=True,\n",
    "                                     decoder_channels=(256, 128, 64, 32, 16),\n",
    "                                     decoder_attention_type=None,\n",
    "                                     in_channels=3,\n",
    "                                     classes=10, activation=None, aux_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54869414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from models import head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a9b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_head=head.MetricLayer(32,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088b9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51a447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_head(torch.zeros(1,32,128,128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20842269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ranger21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranger21.Ranger21()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b413458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d1914f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.sgd.SGD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c20f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
